{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DJz970tEiBb"
      },
      "source": [
        "!git clone https://github.com/joren015/ASLRecognition.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qlea7MleS2wd"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQSPWWS1X3YH"
      },
      "source": [
        "!cp -r /content/ASLRecognition/Datasets /content\r\n",
        "!cp /content/ASLRecognition/Functions.py /content\r\n",
        "!cp /content/ASLRecognition/Model.py /content\r\n",
        "!cp /content/ASLRecognition/NotebookUtils.py /content\r\n",
        "!mkdir /content/Datasets/bronze\r\n",
        "!mkdir /content/Datasets/bronze/homemade_dataset\r\n",
        "!cp -r /content/drive/Shareddrives/CSCI5561_EricksonJorenby_CollectedDataset/CSCI5561_EricksonJorenby_CollectedDataset/StillImageDataset.zip /content\r\n",
        "!unzip /content/StillImageDataset.zip -d /content/Datasets/bronze/homemade_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR4A4OYJEeTP"
      },
      "source": [
        "!pip install mediapipe==0.8.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3baUjVtYZ7vF"
      },
      "source": [
        "from os import walk, makedirs, listdir, cpu_count, environ\r\n",
        "from os.path import join, exists, isfile, join\r\n",
        "import cv2\r\n",
        "\r\n",
        "if not exists(\"/content/Datasets/silver/homemade_dataset/\"):\r\n",
        "    makedirs(\"/content/Datasets/silver/homemade_dataset/\")\r\n",
        "\r\n",
        "count = 0\r\n",
        "dirs = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\"]\r\n",
        "for i in dirs:\r\n",
        "  print(i)\r\n",
        "  image_dir = \"/content/Datasets/bronze/homemade_dataset/{}/\".format(i)\r\n",
        "  files = [\"{}{}\".format(image_dir, x) for x in listdir(image_dir) if x.split('.')[-1] == 'jpg']\r\n",
        "  \r\n",
        "  for f in files:\r\n",
        "    print(f)\r\n",
        "    img = cv2.imread(f)\r\n",
        "    res = cv2.rotate(img, cv2.cv2.ROTATE_90_CLOCKWISE)\r\n",
        "    save_dir = \"/content/Datasets/silver/homemade_dataset/center_2_{}_{}.png\".format(i, count)\r\n",
        "    cv2.imwrite(save_dir, res)\r\n",
        "    count+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq3UWR0aEeJy"
      },
      "source": [
        "from NotebookUtils import PrintDatetime\r\n",
        "PrintDatetime(\"Importing packages\")\r\n",
        "import itertools  \r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import mediapipe as mp\r\n",
        "import shutil\r\n",
        "from os import walk, makedirs, listdir, cpu_count, environ\r\n",
        "from os.path import join, exists, isfile, join\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from Functions import *\r\n",
        "from Model import *\r\n",
        "PrintDatetime()\r\n",
        "\r\n",
        "\r\n",
        "root_dir = \"/content\"\r\n",
        "dataset_path = \"{}/Datasets\".format(root_dir)\r\n",
        "if not exists(root_dir):\r\n",
        "  makedirs(root_dir)\r\n",
        "\r\n",
        "directories = [\"{}/models\", \"{}/Datasets/bronze/homemade_dataset\", \"{}/Datasets/silver/homemade_dataset\", \"{}/Datasets/silver/homemade_dataset_resized\", \"{}/Datasets/gold/Homography\", \"{}/Datasets/gold/Censure\", \"{}/Datasets/gold/Transforms\", \"{}/Datasets/gold/homemade_dataset_resized\", \"{}/Datasets/gold/homemade_dataset_transformed/\"]\r\n",
        "directories = [x.format(root_dir) for x in directories]\r\n",
        "for d in directories:\r\n",
        "  if not exists(d):\r\n",
        "    makedirs(d)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "train_test_path_base = \"{}/gold\".format(dataset_path)\r\n",
        "alpha_num = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\r\n",
        "encoding_config = {\"a\": 0, \"b\": 1,\"c\": 2,\"d\": 3,\"e\": 4,\"f\": 5,\"g\": 6,\"h\": 7\r\n",
        "                   ,\"i\": 8,\"j\": 9,\"k\": 10,\"l\": 11,\"m\": 12,\"n\": 13,\"o\": 14\r\n",
        "                   ,\"p\": 15,\"q\": 16,\"r\": 17,\"s\": 18,\"t\": 19,\"u\": 20,\"v\": 21\r\n",
        "                   ,\"w\": 22,\"x\": 23,\"y\": 24,\"z\": 25,\"0\": 26,\"1\": 27,\"2\": 28\r\n",
        "                   ,\"3\": 29,\"4\": 30,\"5\": 31,\"6\": 32,\"7\": 33,\"8\": 34,\"9\": 35 }\r\n",
        "PrintDatetime()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "PrintDatetime(\"Started\")\r\n",
        "image_dir = \"/content/Datasets/silver/homemade_dataset/\"\r\n",
        "print(image_dir)\r\n",
        "file_list = [\"{}{}\".format(image_dir, x) for x in listdir(image_dir)]\r\n",
        "save_dir = \"/content/Datasets/silver/homemade_dataset_annotated/\"\r\n",
        "landmarks_save_dir = \"/content/Datasets/gold/homemade_dataset_resized/\"\r\n",
        "MPLoop(file_list, save_dir, landmarks_save_dir)\r\n",
        "\r\n",
        "PrintDatetime()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "PrintDatetime(\"Started resizing image dataset\")\r\n",
        "PreprocessDatasets(\"{}/silver/homemade_dataset\".format(dataset_path), \"{}/gold/homemade_dataset_resized\".format(dataset_path), \".png\", overwrite=True)\r\n",
        "PrintDatetime()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exHc4SMup4fY"
      },
      "source": [
        "from os import remove\r\n",
        "\r\n",
        "load_dir = \"/content/Datasets/gold/homemade_dataset_resized\"\r\n",
        "\r\n",
        "dirs = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\"]\r\n",
        "for c in dirs:\r\n",
        "  image_paths = []\r\n",
        "  files_paths = []\r\n",
        "  image_paths += [\"{}/{}\".format(load_dir, x) for x in listdir(load_dir) if x.split('.')[-1] == \"png\" and x.split(\"_\")[2] == c]\r\n",
        "  files_paths += [\"{}/{}\".format(load_dir, y) for y in listdir(load_dir) if y.split('.')[-1] == \"pt\" and y.split(\"_\")[2] == c]\r\n",
        "  print(image_paths)\r\n",
        "  print(files_paths)\r\n",
        "  \r\n",
        "  for image_i in image_paths:\r\n",
        "    landmarks_i = image_i.replace(\".png\", \"_landmarks.pt\")\r\n",
        "    if landmarks_i not in files_paths:\r\n",
        "      rem_pt = image_i.replace(\".png\", \".pt\")\r\n",
        "      remove(image_i)\r\n",
        "      remove(rem_pt)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "PrintDatetime(\"Started determining new baseline\")\r\n",
        "baseline_results_path = \"{}/bronze/homemade_dataset\".format(dataset_path)\r\n",
        "DetermineNewBaseline(\"{}/gold/homemade_dataset_resized\".format(dataset_path), baseline_results_path)\r\n",
        "PrintDatetime()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVT1KdCxkMJe"
      },
      "source": [
        "if not exists(\"/content/Datasets/gold/homemade_dataset_transformed/homography/\"):\r\n",
        "    makedirs(\"/content/Datasets/gold/homemade_dataset_transformed/homography/\")\r\n",
        "if not exists(\"/content/Datasets/gold/homemade_dataset_transformed/censure/\"):\r\n",
        "    makedirs(\"/content/Datasets/gold/homemade_dataset_transformed/censure/\")\r\n",
        "\r\n",
        "PrintDatetime(\"Started transforming video dataset\")\r\n",
        "\r\n",
        "dirs = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\"]\r\n",
        "results = {c: {} for c in dirs}\r\n",
        "transforms = [\"homography\", \"CENSURE\"]\r\n",
        "for transform in transforms:\r\n",
        "  if transform==\"homography\":\r\n",
        "    baseline2 = NewBaseline(baseline_results_path + \"/baseline_eval_homography_homemade_dataset.json\")\r\n",
        "  if transform==\"CENSURE\":\r\n",
        "    baseline2 = NewBaseline(baseline_results_path + \"/baseline_eval_CENSURE_homemade_dataset.json\")\r\n",
        "\r\n",
        "  for c in dirs:\r\n",
        "    image_paths = []\r\n",
        "    image_dir = \"{}/gold/homemade_dataset_resized\".format(dataset_path)\r\n",
        "    image_paths += [\"{}/{}\".format(image_dir, x) for x in listdir(image_dir) if x.split('.')[-1] == \"png\" and x.split(\"_\")[2] == c]\r\n",
        "    baseline_img_path = [x for x in baseline2 if x.split('/')[-1].split('_')[2] == c][0]\r\n",
        "    baseline_fp_path = baseline_img_path.replace(\".png\", \"_landmarks.pt\")\r\n",
        "    baseline_img = cv2.imread(baseline_img_path)\r\n",
        "    baseline_fp = GetPoints(baseline_img_path, baseline_fp_path)\r\n",
        "    for img_path in image_paths:\r\n",
        "      if img_path != baseline_img_path:\r\n",
        "        img_fp_path = img_path.replace(\".png\", \"_landmarks.pt\")\r\n",
        "        img = cv2.imread(img_path)\r\n",
        "        img_fp = GetPoints(img_path, img_fp_path)\r\n",
        "        if transform == \"homography\":\r\n",
        "          img_transformed = HomographyTransform(img, baseline_img, img_fp, baseline_fp)\r\n",
        "          cv2.imwrite(img_path.replace(\"homemade_dataset_resized\", \"homemade_dataset_transformed/homography\"), img_transformed)\r\n",
        "          torch.save(torch.from_numpy(img_transformed.reshape(3, 200, 200)), img_path.replace(\"homemade_dataset_resized\", \"homemade_dataset_transformed/homography\").replace(\".png\", \".pt\"))\r\n",
        "        if transform == \"CENSURE\":\r\n",
        "          img_transformed = CENSURETransform(img, baseline_img, img_fp, baseline_fp)\r\n",
        "          #cv2.imwrite(img_path.replace(\"homemade_dataset_resized\", \"homemade_dataset_transformed/censure\"), img_transformed)\r\n",
        "          #torch.save(torch.from_numpy(img_transformed.reshape(3, 200, 200)), img_path.replace(\"homemade_dataset_resized\", \"homemade_dataset_transformed/censure\").replace(\".png\", \".pt\"))\r\n",
        "        \r\n",
        "\r\n",
        "    print(image_paths)\r\n",
        "    print(\"Character: {}\".format(c))\r\n",
        "\r\n",
        "PrintDatetime()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfH3vjlk3Ahi"
      },
      "source": [
        "PrintDatetime(\"Started labeling non-transformed image data\")\r\n",
        "LabelDatasets(load_path=\"{}/gold/homemade_dataset_resized\".format(dataset_path), save_path=\"{}/gold/homemade_dataset_resized\".format(dataset_path), encoding_config=encoding_config)\r\n",
        "PrintDatetime()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "PrintDatetime(\"Started labeling annotations transformed image data\")\r\n",
        "LabelDatasets(load_path=\"{}/gold/homemade_dataset_transformed/homography\".format(dataset_path), save_path=\"{}/gold/homemade_dataset_transformed/homography\".format(dataset_path), encoding_config=encoding_config)\r\n",
        "PrintDatetime()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "PrintDatetime(\"Started labeling CENSURE transformed image data\")\r\n",
        "LabelDatasets(load_path=\"{}/gold/homemade_dataset_transformed/censure\".format(dataset_path), save_path=\"{}/gold/homemade_dataset_transformed/censure\".format(dataset_path), encoding_config=encoding_config)\r\n",
        "PrintDatetime()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y9esdgzJs2u"
      },
      "source": [
        "PrintDatetime(\"Started training\")\r\n",
        "hpt = {\"learning_rate\": [1e-2, 1e-3, 1e-4, 1e-5], \"batch_size\": [8, 32, 128]}\r\n",
        "keys, values = zip(*hpt.items())\r\n",
        "num_epoch = 1000\r\n",
        "i = 1\r\n",
        "for v in itertools.product(*values):\r\n",
        "  model_save_path_base = \"/content/models/HomemadeDataset/Base/{}\".format(i)\r\n",
        "  model_save_path_transformed = \"/content/models/HomemadeDataset/Transformed/{}\".format(i)\r\n",
        "  model_save_path_cross = \"/content/models/HomemadeDataset/Cross/{}\".format(i)\r\n",
        "  makedirs(model_save_path_base)\r\n",
        "  makedirs(model_save_path_transformed)\r\n",
        "  makedirs(model_save_path_cross)\r\n",
        "  experiment = dict(zip(keys, v))\r\n",
        "  print(experiment)\r\n",
        "  learning_rate = experiment[\"learning_rate\"]\r\n",
        "  batch_size = experiment[\"batch_size\"]\r\n",
        "\r\n",
        "  PrintDatetime(\"Started\")\r\n",
        "  homemade_train_test_path = \"{}/homemade_dataset_resized\".format(train_test_path_base)\r\n",
        "  full_dataset = pd.read_csv(\"{}/labels.csv\".format(homemade_train_test_path))\r\n",
        "  az_dataset = full_dataset[full_dataset[\"LabelEncoded\"] <= 25]\r\n",
        "  train_dataset, test_val_dataset = train_test_split(az_dataset, test_size=0.2, random_state=0, shuffle=True)\r\n",
        "  test_dataset, val_dataset = train_test_split(test_val_dataset, test_size=0.5, random_state=0, shuffle=True)\r\n",
        "  train_dataset.to_csv(\"{}/train_az.csv\".format(homemade_train_test_path))\r\n",
        "  test_dataset.to_csv(\"{}/test_az.csv\".format(homemade_train_test_path))\r\n",
        "  val_dataset.to_csv(\"{}/val_az.csv\".format(homemade_train_test_path))\r\n",
        "  main(\"{}/train_az.csv\".format(homemade_train_test_path), \"{}/test_az.csv\".format(homemade_train_test_path), \"{}/val_az.csv\".format(homemade_train_test_path), learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epoch, model_save_path=model_save_path_base)\r\n",
        "  PrintDatetime()\r\n",
        "\r\n",
        "  homemade_train_test_path_transformed = \"{}/homemade_dataset_transformed\".format(train_test_path_base)\r\n",
        "  full_dataset = pd.read_csv(\"{}/labels.csv\".format(homemade_train_test_path_transformed))\r\n",
        "  az_dataset = full_dataset[full_dataset[\"LabelEncoded\"] <= 25]\r\n",
        "  train_dataset, test_val_dataset = train_test_split(az_dataset, test_size=0.2, random_state=0, shuffle=True)\r\n",
        "  test_dataset, val_dataset = train_test_split(test_val_dataset, test_size=0.5, random_state=0, shuffle=True)\r\n",
        "  train_dataset.to_csv(\"{}/train_az.csv\".format(homemade_train_test_path_transformed))\r\n",
        "  test_dataset.to_csv(\"{}/test_az.csv\".format(homemade_train_test_path_transformed))\r\n",
        "  val_dataset.to_csv(\"{}/val_az.csv\".format(homemade_train_test_path_transformed))\r\n",
        "  PrintDatetime(\"Started training experiment\")\r\n",
        "  main(\"{}/train_az.csv\".format(homemade_train_test_path_transformed), \"{}/test_az.csv\".format(homemade_train_test_path_transformed), \"{}/val_az.csv\".format(homemade_train_test_path_transformed), learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epoch, model_save_path=model_save_path_transformed)\r\n",
        "  PrintDatetime()\r\n",
        "\r\n",
        "  PrintDatetime(\"Started training experiment\")\r\n",
        "  main(\"{}/train_az.csv\".format(homemade_train_test_path), \"{}/test_az.csv\".format(homemade_train_test_path_transformed), \"{}/val_az.csv\".format(homemade_train_test_path_transformed), learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epoch, model_save_path=model_save_path_cross)\r\n",
        "  PrintDatetime()\r\n",
        "\r\n",
        "\r\n",
        "  i += 1\r\n",
        "\r\n",
        "PrintDatetime()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}